This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-16T03:05:17.314Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
static/
  index.html
.gitignore
app.py
package.json
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="static/index.html">
<!DOCTYPE html>
<html>
<head>
    <title>Lightweight Diffusion Showcase</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .input-group {
            margin-bottom: 20px;
        }
        input[type="text"] {
            width: 100%;
            padding: 8px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-bottom: 10px;
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        button:disabled {
            background: #ccc;
        }
        .canvas-container {
            width: 512px;
            height: 512px;
            margin: 20px auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            overflow: hidden;
        }
        #status {
            text-align: center;
            margin: 10px 0;
            color: #666;
        }
        .progress {
            height: 4px;
            background: #eee;
            margin: 10px 0;
            border-radius: 2px;
            overflow: hidden;
        }
        .progress-bar {
            height: 100%;
            background: #007bff;
            width: 0%;
            transition: width 0.3s ease;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Lightweight Diffusion Showcase</h1>
        <div class="input-group">
            <input type="text" id="prompt" placeholder="Enter your prompt here..." />
            <button id="generate">Generate</button>
        </div>
        <div class="progress">
            <div class="progress-bar" id="progress"></div>
        </div>
        <div id="status">Ready to generate</div>
        <div class="canvas-container">
            <canvas id="canvas" width="512" height="512"></canvas>
        </div>
    </div>
    <script>
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const generateBtn = document.getElementById('generate');
        const promptInput = document.getElementById('prompt');
        const progressBar = document.getElementById('progress');
        const statusText = document.getElementById('status');
        function updateProgress(step, total) {
            const percent = (step / total) * 100;
            progressBar.style.width = `${percent}%`;
            statusText.textContent = `Generating: Step ${step} of ${total}`;
        }
        function displayImage(base64Image) {
            const img = new Image();
            img.onload = () => {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(img, 0, 0, canvas.width, canvas.height);
            };
            img.src = 'data:image/png;base64,' + base64Image;
        }
        generateBtn.addEventListener('click', async () => {
            const prompt = promptInput.value.trim();
            if (!prompt) return;
            generateBtn.disabled = true;
            statusText.textContent = 'Starting generation...';
            progressBar.style.width = '0%';
            try {
                const response = await fetch('http://localhost:8080/generate', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ prompt })
                });
                const data = await response.json();
                // Display each step
                for (let i = 0; i < data.steps.length; i++) {
                    const step = data.steps[i];
                    updateProgress(step.step, 10);
                    displayImage(step.image);
                    await new Promise(resolve => setTimeout(resolve, 200)); // Small delay between steps
                }
                // Display final image
                displayImage(data.final_image);
                statusText.textContent = 'Generation complete!';
                progressBar.style.width = '100%';
            } catch (error) {
                statusText.textContent = 'Error: ' + error.message;
            } finally {
                generateBtn.disabled = false;
            }
        });
        // Clear canvas with gray background initially
        ctx.fillStyle = '#f0f0f0';
        ctx.fillRect(0, 0, canvas.width, canvas.height);
    </script>
</body>
</html>
</file>

<file path=".gitignore">
# Dependencies
node_modules/
package-lock.json
yarn.lock
pnpm-lock.yaml

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/
.env
.venv
pip-log.txt
pip-delete-this-directory.txt
.python-version

# Environment variables
.env
.env.local
.env.*.local
.cursor-tools.env

# Build outputs
dist/
build/
out/
*.tsbuildinfo

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# IDE and editor files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
Thumbs.db

# Testing
coverage/
.nyc_output/
.pytest_cache/
.coverage
htmlcov/

# Temporary files
*.tmp
*.temp
.cache/

# Local documentation and research
local-docs/
local-research/

# Debug
.debug/
debug.log

# Optional: Playwright
test-results/
playwright-report/
playwright/.cache/

# ML Models and data
*.pt
*.pth
*.onnx
*.h5
model_weights/
datasets/
checkpoints/
</file>

<file path="app.py">
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import torch
from diffusers import StableDiffusionPipeline
from PIL import Image
import io
import base64
import numpy as np
app = Flask(__name__)
CORS(app)
# Initialize the small model
model_id = "madebyollin/sdxl-tiny"
pipe = None
def load_model():
    global pipe
    if pipe is None:
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            use_safetensors=True  # More memory efficient
        )
        if torch.cuda.is_available():
            pipe = pipe.to("cuda")
        pipe.enable_attention_slicing()
        pipe.enable_model_cpu_offload()  # Further reduce memory usage
def get_image_base64(image):
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str
@app.route('/generate', methods=['POST'])
def generate():
    if pipe is None:
        load_model()
    data = request.json
    prompt = data.get('prompt', '')
    current_step = []
    def callback(step, timestep, latents):
        with torch.no_grad():
            image = pipe.decode_latents(latents.detach())
            image = pipe.numpy_to_pil(image)[0]
            current_step.append({
                'step': step,
                'image': get_image_base64(image)
            })
    # Generate image with minimal steps
    image = pipe(
        prompt,
        num_inference_steps=10,
        callback=callback,
        callback_steps=1
    ).images[0]
    return jsonify({
        'steps': current_step,
        'final_image': get_image_base64(image)
    })
@app.route('/')
def index():
    return send_file('static/index.html')
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8080)
</file>

<file path="package.json">
{
  "name": "cursor-tools-test",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "cursor-tools": "latest"
  }
}
</file>

<file path="README.md">
# Lightweight Diffusion Showcase

A minimal web application that demonstrates the diffusion model image generation process in real-time. This implementation uses a lightweight version of Stable Diffusion optimized for performance.

## Features

- Real-time visualization of the diffusion process
- Lightweight model implementation (Small Stable Diffusion V0)
- Simple and clean user interface
- Progress tracking for each generation step
- Minimal dependencies

## Setup

1. Create a virtual environment (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the application:
```bash
python app.py
```

4. Open your browser and navigate to:
```
http://localhost:5000
```

## Usage

1. Enter your prompt in the text input field
2. Click "Generate" to start the diffusion process
3. Watch as the image gradually forms through each step
4. The final image will be displayed when generation is complete

## Technical Details

- Uses the Small Stable Diffusion V0 model (50% smaller than standard SD)
- Optimized for minimal memory usage with attention slicing
- 10 inference steps for faster generation
- Real-time visualization of each step
- Lightweight frontend with vanilla JavaScript

## Requirements

- Python 3.7+
- PyTorch
- Flask
- Modern web browser

## Notes

- First generation may take longer as the model needs to be loaded
- Generation speed depends on your hardware capabilities
- GPU is recommended but not required
</file>

<file path="requirements.txt">
flask==2.3.3
flask-cors==3.0.10
torch==2.2.1
diffusers==0.24.0
transformers==4.36.2
numpy>=1.26.0
pillow==10.0.0
</file>

</files>
